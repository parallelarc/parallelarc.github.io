[
  {
    "id": 3894502715,
    "number": 8,
    "title": "范浩强高阳 - 决策备忘录",
    "content": "**访谈来源**：晚点聊 LateTalk 第149期\n**嘉宾**：范浩强（原力灵机联创）、高阳（千寻智能联创兼首席科学家）\n\n---\n\n## 一、具身智能测评：从Demo工程到科学评测\n\n### RoboChallenge平台设计理念\n\n- Robotics研究长期依赖cherry-pick的demo视频，录100遍取1条成功展示\n- 学术界最佳论文也只测试3-4个任务，且不同论文任务不统一\n- 真机测试方差大，需大规模重复测试（数百次）才能控制方差\n- RoboChallenge采用Fine-tune设定：30个任务，每个任务约1000条示范数据\n\n### Table30任务设计逻辑\n\n- 30个任务由内部研究员\"画钩\"设计，事后分析考点分布合理\n- 每个任务有独特难点，考点覆盖丰富\n- 碎纸任务：纸遮挡手眼，考验克服视觉盲区能力\n- 插花任务：从执行器-物体互动扩展到物体-物体互动\n- 扫二维码任务：扫前后图像状态一致，暴露单帧模型记忆短板\n\n### Pi 0到Pi 0.5的跨越式进步\n\n- Pi 0在Table30上成功率仅20%多，平均4次尝试成功1次\n- Pi 0.5提升至42%左右，简单任务可做到100%成功\n- 国产千寻Spirit V1.5超越Pi 0.5登顶榜单\n- 从旁观者视角能明显感受到模型\"更灵光\"，行业处于具体进展阶段\n\n### 两种测评范式对比\n\n- RoboArena：Zero Shot设定，假设模型足够强可直接执行\n- RoboChallenge：Fine-tune设定，符合当前实际需求\n- 当前大多数模型在Zero Shot下成功率接近零，无法有效对比\n- 测评目标从\"百分之一精度\"转向\"反映模型发展趋势\"，能区分明显代差即可\n\n---\n\n## 二、具身智能的核心瓶颈：Scaling Data\n\n### 数据是当前最大瓶颈\n\n- 若有像大模型般无穷多的数据，具身模型技术路径已较清晰\n- 2026年具身智能最核心主题是\"Scaling Data\"\n- 瓶颈不在技术原理，而在规模化、低成本获取有效数据\n\n### 四种数据获取路径\n\n- **仿真数据**：需艺术家在仿真器中搭建场景，扩展缓慢，多样性难以提升\n- **人类视频数据**：从人类操作视频中学习，效率高\n- **可穿戴设备采集**：工作者佩戴设备采集真实场景，可达千万小时级别\n- **遥操作数据**：真机远程操作，质量最高但成本高（需建造机器人）\n\n### 数据路径的战略选择\n\n- 千寻选择：人类视频 + 可穿戴 + 遥操作，排除仿真\n- 原力灵机选择：以真机为主，复用旷视时期的大规模线下采集体系经验\n- Generalist AI策略：人拿夹子采集，已采27万小时，每周新增1万小时\n- 条条大路通罗马，最终卡点可能相同\n\n### 仿真数据的困境\n\n- 每个仿真场景需人工搭建，耗时缓慢\n- 需持续投入3D资产制作、大规模资产扫描\n- 当前多样性提升存在瓶颈，但未来可能成为重要方向\n\n---\n\n## 三、VLA模型的记忆缺失问题\n\n### 单帧模型的失忆困境\n\n- 大多数开源VLA基于单帧，无记忆能力\n- 模型每0.几秒就\"失忆\"，类似\"每7秒就忘\"的金鱼\n- 只能看到当前场景，无法记住之前执行的动作\n\n### 记忆短板的实战暴露\n\n- 扫二维码任务：拿起扫码枪扫前扫后图像状态一致\n- 模型无法判断是否已扫码，常常一伸手后停止不动\n- 需记忆能力支持：模型需记住\"刚才干了什么\"\n\n### 下一个关键突破点\n\n- 记忆能力是VLA模型的必经之路\n- 部分研究已开始将记忆机制引入模型\n- 从单帧向多帧+记忆演进是技术趋势\n\n---\n\n## 四、具身智能的GPT-3时刻\n\n### 当前发展阶段\n\n- 类比大模型，具身智能处于\"视觉AlexNet时期\"\n- 历史是波波echo，且echo频率越来越快\n- 处于加速进化前夜\n\n### 标志性任务：叠被子\n\n- 扫地机器人厂商和家电厂不会认为叠被子是该做的事\n- 任务有一定用处且不那么简单\n- 旧技术（检测分割）无法完成，具身智能的突破口\n- Pi 0.6可叠纸盒，Dyna Robotics主攻叠毛巾等软物体操作\n\n### 时间的双重感知\n\n- 从业者看到早期信号的时间比普通人早得多\n- 普通人看到破圈需3-4年\n- 某些任务对从业者虽不惊艳但已表明临界点临近\n- 硬件精细度和给力程度是软物体操作的关键制约\n\n---\n\n## 五、Demo工程：行业公开的秘密\n\n### 四种造假方式\n\n- **Cherry pick**：录100遍取1条成功展示\n- **视频剪辑**：后期剪辑加速，掩盖实际耗时\n- **遥操作**：背后是人远程操作，非模型自主执行\n- **AIGC**：直接生成虚假演示视频\n\n### 防作弊机制\n\n- 放置iPad时钟防止剪辑加速\n- Demo时放置随机哈希值证明视频唯一性\n- 规则取最后一次提交成绩，防止多次提交取最优\n- 在线测评环境下无法控制物理环境，作弊成本上升\n\n### 唯一可靠验证方法\n\n- 现场观摩是辨别demo真伪的唯一去处\n- 任何视频都可能被精心制作\n- 行业对\"demo工程\"心照不宣但很少公开讨论\n\n---\n\n## 六、2026年中美具身竞争格局\n\n### 中国能否实现具身DeepSeek时刻\n\n- 过去看国外工作如Google只能羡慕\n- 视觉时代人脸识别：Google从\"天外来物\"到国内追上仅用3年\n- 当前节奏更快，具身领域可能更快实现追赶\n- 2026年可能见证中国在具身领域超越美国\n\n### 自信度随技术路线清晰化提升\n\n- 创业初期需不断说服自己\"时刻已到\"\n- 随着技术路线收敛，疑惑变少，确定性增加\n- 2026年核心悬念：具身基础模型能否达到GPT-3或GPT-3.5水平\n- 行业进步具体可见，信心建立在实测数据而非demo视频\n\n---\n\n## 核心术语\n\n- **VLA**：Vision-Language-Action模型，具身智能主流技术路线\n- **Fine-tune**：使用少量示范数据对基础模型进行微调\n- **Zero Shot**：零样本，无需示例数据直接执行任务\n- **Cherry pick**：从多次尝试中挑选最成功的展示\n- **Table30**：RoboChallenge的30个桌面操作任务\n- **RoboChallenge**：原力灵机与Hugging Face发起的Fine-tune测评平台\n- **RoboArena**：Physical Intelligence发起的Zero Shot测评平台\n- **Pi**：Physical Intelligence的具身模型系列\n- **Demo工程**：精心挑选、剪辑、优化的演示视频\n",
    "createdAt": "2026-02-04T04:18:00Z",
    "updatedAt": "2026-02-04T05:11:12Z",
    "author": "parallelarc",
    "authorAvatar": "https://avatars.githubusercontent.com/u/63178075?v=4",
    "authorUrl": "https://github.com/parallelarc",
    "labels": ["blog", "podcast"],
    "url": "https://github.com/parallelarc/parallelarc.github.io/issues/8",
    "commentsCount": 0,
    "excerpt": "访谈来源：晚点聊 LateTalk 第149期 嘉宾：范浩强（原力灵机联创）、高阳（千寻智能联创兼首席科学家） ---  一、具身智能测评：从Demo工程到科学评测  RoboChallenge平台设计理念 - Robotics研究长期依赖cherry-pick的demo视频，录100遍取1条成功展..."
  },
  {
    "id": 3870737295,
    "number": 7,
    "title": "Demystifying evals for AI agents",
    "content": "# Demystifying evals for AI agents\n\n**来源**: [Demystifying evals for AI agents](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents) | Anthropic Engineering\n\n---\n\n## 评估体系的核心概念\n\n- **评估(Eval)**：给AI输入，应用评分逻辑测量输出的测试系统\n- **单轮评估**：一次提示、一次响应、一次评分—适用于早期LLM测试\n- **多轮评估**：Agent跨越多个轮次调用工具、修改状态、适应中间结果\n- **Task（任务）**：具有定义输入和成功标准的单个测试用例\n- **Trial（试验）**：对任务的单次尝试—多次试验产生更一致的结果\n- **Grader（评分器）**：对Agent性能某方面进行评分的逻辑\n- **Transcript（记录）**：试验的完整记录—包含输出、工具调用、推理、中间结果\n- **Outcome（结果）**：试验结束时环境的最终状态—区别于Agent声称的状态\n- **Evaluation Harness**：端到端运行评估的基础设施\n- **Agent Harness**：使模型能够作为Agent运行的系统（如Claude Code）\n- **Evaluation Suite**：为测量特定能力而设计的任务集合\n\n**关键洞察**：Agent评估的核心复杂性来自多轮交互—错误会传播和累积，且前沿模型可能找到超越静态评估限制的创造性解决方案\n\n---\n\n## 构建评估的时机与价值\n\n- **早期构建**：在原型阶段即可开始，20-50个简单任务即可起步\n- **效应量原理**：早期Agent开发的每个系统变更都有明显影响，小样本量足够检测\n- **明确成功定义**：评估迫使产品团队明确定义Agent的成功标准\n- **避免反应式循环**：无评估时团队陷入\"等待投诉→手动重现→修复→祈祷无回归\"的循环\n- **加速模型采用**：有评估的团队可在数天内确定新模型优势并升级，无评估团队需数周测试\n- **免费获得基线**：一旦建立评估，自动获得延迟、Token使用、每任务成本、错误率的跟踪基准\n- **产品-研究沟通**：评估成为产品与研究团队之间最高带宽的沟通渠道\n\n**关键洞察**：评估的价值随时间复合—成本前期可见，收益后期累积，容易被低估\n\n---\n\n## 评分器类型与选择框架\n\n### 代码评分器\n- **适用场景**：确定性测试、单元测试、静态分析、状态验证\n- **优势**：快速、廉价、客观、可重现、易于调试\n- **局限**：对不符合精确模式的有效变体脆弱、缺乏细微差别\n- **方法**：字符串匹配（精确/正则/模糊）、二元测试、Lint/类型/安全检查\n\n### 模型评分器\n- **适用场景**：开放式任务、自由形式输出、需要细微差别的判断\n- **优势**：灵活、可扩展、捕捉细微差别、处理开放性输出\n- **局限**：非确定性、比代码昂贵、需要与人工评分器校准\n- **方法**：基于量表的评分、自然语言断言、成对比较、参考基准评估、多评委共识\n\n### 人工评分器\n- **适用场景**：校准模型评分器、主观性强的任务、最终质量验证\n- **优势**：金标准质量、匹配专家用户判断\n- **局限**：昂贵、缓慢、需要大规模专家访问\n- **方法**：SME审查、众包判断、抽样检查、A/B测试\n\n**选择原则**：代码评分器优先→模型评分器补充→人工评分器校准\n\n---\n\n## 不同Agent类型的评估策略\n\n### 编程Agent\n- **核心方法**：确定性测试为主—单元测试通过即成功\n- **SWE-Bench Verified**：从GitHub仓库获取issue，通过运行测试套件评分\n- **Terminal-Bench**：测试端到端技术任务（如从源码构建Linux内核）\n- **评分维度**：测试通过性+代码质量规则+工具调用行为\n- **关键实践**：稳定测试环境+完善测试用例+生成代码的静态分析\n\n### 对话Agent\n- **核心挑战**：交互质量本身是评估对象\n- **多维成功**：工单解决（状态检查）+<10轮对话（记录约束）+语气适当（LLM量表）\n- **τ-Bench/τ²-Bench**：模拟多轮交互—一个模型扮演用户角色\n- **评分方法**：LLM量表评估任务完成+交互质量+第二LLM模拟用户\n- **注意**：许多任务有多种\"正确\"解决方案，避免刚性路径评分\n\n### 研究Agent\n- **核心挑战**：质量只能相对于任务判断—\"全面\"、\"有据\"、\"正确\"标准因场景而异\n- **评分组合**：基础性检查（声明有来源支持）+覆盖检查（必须包含的关键事实）+来源质量检查\n- **BrowseComp**：测试Agent能否在开放网络中找到\"大海捞针\"式答案\n- **关键实践**：LLM量表需频繁与专家人工判断校准\n- **客观答案**：对有客观正确答案的任务使用精确匹配\n\n### 计算机使用Agent\n- **评估环境**：真实或沙箱环境中运行Agent，检查是否达到预期结果\n- **WebArena**：基于浏览器的任务—URL和页面状态检查+后端状态验证\n- **OSWorld**：完整操作系统控制—检查文件系统状态、应用配置、数据库内容、UI元素属性\n- **效率权衡**：DOM交互快但Token密集，截图交互慢但Token高效\n- **实践案例**：Claude for Chrome开发评估检查Agent是否选择正确工具\n\n---\n\n## 非确定性评估的度量指标\n\n- **pass@k**：k次尝试中至少一次成功的概率\n  - k增加时pass@k上升—更多射门机会提高至少一次成功几率\n  - 适用场景：工具类Agent，一次成功即有价值\n- **pass^k**：所有k次试验都成功的概率\n  - k增加时pass^k下降—要求更多次一致成功是更高门槛\n  - 计算：单次75%成功率，3次试验全部成功概率=0.75³≈42%\n  - 适用场景：面向客户的Agent，用户期望每次都可靠\n- **指标选择**：根据产品需求选择—pass@k用于一次成功重要，pass^k用于一致性关键\n\n**关键洞察**：k=1时两者相同（等于单次成功率）；k=10时两者讲述相反故事—pass@k接近100%而pass^k趋近0%\n\n---\n\n## 从零到一的评估构建路线图\n\n### 收集初始任务\n1. **从20-50个简单任务开始**—团队误以为需要数百个任务而延迟构建\n2. **已有手动检查优先**：开发中手动验证的行为+最终用户尝试的常见任务\n3. **从失败收集**：生产环境中查看Bug跟踪器和支持队列，将用户报告的失败转为测试用例\n4. **明确任务规格**：两个领域专家应能独立达成相同通过/失败判断\n5. **创建参考解决方案**：证明任务可解+验证评分器正确配置\n\n### 构建平衡问题集\n- **双向测试**：同时测试行为应该发生和不应该发生的场景\n- **避免类别不平衡**：单向评估导致单向优化\n- **实战案例**：Claude.ai网络搜索评估—应搜索查询（如天气）+ 不应搜索查询（如\"谁创立了Apple\"）\n- **避免模糊规格**：评分器检查的所有内容应从任务描述中清晰可见\n\n### 设计评估Harness与评分器\n- **环境隔离**：每次试验从干净环境开始—避免共享状态导致的相关失败\n- **生产一致性**：评估中的Agent功能应与生产环境中的Agent大致相同\n- **评分稳定**：环境本身不应引入额外噪声\n- **优先结果评分**：评估Agent产出了什么，而非采取的路径—Agent经常找到评估设计者未预期的有效方法\n- **部分信用**：对多组件任务建立部分信用机制\n- **LLM校准**：LLM评委应与专家人工判断密切校准\n\n### 长期维护与使用\n- **阅读记录**：投资工具查看评估记录，定期阅读以验证评分器工作良好\n- **公平失败**：失败应显得公平—清楚Agent哪里出错及为何出错\n- **监控饱和**：100%通过率的评估只跟踪回归，不提供改进信号\n- **持续迭代**：评估套件是需要持续关注的活体工件\n- **领域专家贡献**：最接近产品要求和用户的人最适合定义成功\n\n---\n\n## 评估与其他方法的协同\n\n| 方法 | 最佳使用阶段 | 核心价值 |\n|------|-------------|----------|\n| **自动化评估** | 发布前/CI/CD | 无用户影响快速迭代、每次提交运行、大规模测试场景 |\n| **生产监控** | 发布后 | 揭示真实用户行为、捕捉合成评估遗漏的问题 |\n| **A/B测试** | 有足够流量时 | 衡量真实用户结果（留存、任务完成）、控制混杂因素 |\n| **用户反馈** | 持续 | 揭示未预期问题、附带真实用户案例 |\n| **手动记录审查** | 持续 | 建立失败模式直觉、捕捉自动化检查遗漏的微妙质量问题 |\n| **系统性人工研究** | 校准阶段 | LLM评分器校准、主观输出评估 |\n\n**瑞士奶酪模型**：没有单一评估层能捕获所有问题，多层结合使通过一层的失败被另一层捕获\n\n---\n\n## 评估框架选择\n\n- **Harbor**：容器化环境运行Agent，跨云提供商大规模运行试验，标准化任务和评分器格式\n- **Promptfoo**：轻量灵活开源框架，声明式YAML配置，断言类型从字符串匹配到LLM量表\n- **Braintrust**：离线评估与生产可观测性结合的平台，autoevals库包含预建评分器\n- **LangSmith**：跟踪、离线/在线评估、数据集管理，与LangChain生态系统紧密集成\n- **Langfuse**：自托管开源替代方案，适合有数据驻留要求的团队\n\n**框架建议**：快速选择适合工作流程的框架，将精力投入迭代高质量测试用例和评分器\n\n---\n\n## 核心决策要点\n\n- **评估是核心组件而非事后补充**：投资早期开发加速，投资延迟导致反应式循环\n- **小样本起步**：20-50个简单任务足以开始，效应量原理支持早期小样本量\n- **明确成功标准**：评估是产品需求的压力测试，定义评估任务是验证要求是否足够具体的最佳方式\n- **阅读记录**：不阅读记录无法知道评分器是否工作良好，这是Agent开发的关键技能\n- **组合评分器**：代码评分器优先，模型评分器灵活补充，人工评分器校准\n- **评估驱动开发**：先构建评估定义计划能力，然后迭代直到Agent表现良好\n- **监控饱和**：接近饱和的评估需要扩展—高通过率的能力评估可\"毕业\"成为回归套件\n- **多方法验证**：自动化评估+生产监控+定期人工审查提供最完整的图景\n",
    "createdAt": "2026-01-29T13:37:52Z",
    "updatedAt": "2026-01-29T13:37:52Z",
    "author": "parallelarc",
    "authorAvatar": "https://avatars.githubusercontent.com/u/63178075?v=4",
    "authorUrl": "https://github.com/parallelarc",
    "labels": ["blog", "agent", "evaluation"],
    "url": "https://github.com/parallelarc/parallelarc.github.io/issues/7",
    "commentsCount": 0,
    "excerpt": "Demystifying evals for AI agents 来源: Demystifying evals for AI agents(https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents) | Anthro..."
  },
  {
    "id": 3863709777,
    "number": 6,
    "title": "印奇访谈 - 决策备忘录",
    "content": "# 印奇访谈决策备忘录\n\n---\n\n## 基础模型商业模式\n\n### 不可行的路径\n\n- 基模+纯2B不成立：年投入30-50亿，但2B变现周期长、收入上限有限\n- 基模+纯C端软件不成立：大厂拥有用户和数据飞轮，创业公司难以建立壁垒\n- ChatGPT非终极产品形态，交互仍不自然\n\n### 可行的路径\n\n- AI与终端（人车家）软硬结合是创业公司可行路径\n- 硬件市场难以垄断，多品牌共存给创业公司留出空间\n- 车是第一切口，机器人和穿戴设备是后续方向\n\n---\n\n## 战略取舍原则\n\n- AI 1.0最大教训：多线作战导致压强不够，应在\"想做、能做、可做\"三者交集处聚焦\n- 若重来，应16-17年all-in智驾，不打安防战场\n- 安防是2G市场，营销占比高，与纯技术团队基因不匹配\n- 上市不应成为目标，本质是业务-产品-利润的良性循环\n- 慢就是快：战略聚焦比快速扩张更重要\n\n---\n\n## 技术路径\n\n### 方向判断\n\n- AGI必须与物理世界交互，纯数字世界无法实现真正智能\n- 基模必须世界一流，否则后续无根基\n- 全模态（文字/语音/图像）是人机交互基础\n- VLA（Vision-Language-Action）是面向终端的差异化\n\n### 数据战略\n\n- 数据决定模型70-80%效果，数据汇报线应直连算法负责人\n- 数字空间与物理空间数据需融合于同一模型\n- 室内交互数据稀缺，物理数据收集需5-7年\n\n---\n\n## 组织与人才\n\n### 选人\n\n- 技术能力+协同能力+使命感+小ego，缺一不可\n- 做冷板凳能力是技术信仰的核心体现\n- 聪明人陷阱：想走捷径，但正确方式往往是笨办法\n\n### 架构\n\n- 数据、算法、系统工程三要素必须融合\n- 算法与系统同学交叉培养，不能割裂\n- AI组织需融合top-down（大项目资源集中）和bottom-up（创新活力）\n\n---\n\n## 竞争格局\n\n### 行业判断\n\n- 智驾将头部化为3-4家供应商，技术路径已清晰（模型驱动），窗口期约3年\n- 大模型竞赛赛程过半，进入淘汰赛\n- 过去10年最成功的AI公司是字节和特斯拉（从场景切入，非技术出发）\n\n### 应对策略\n\n- 创业公司需与巨头局部竞争，非全面对抗\n- 找到长板和切口，专注单一领域深耕\n",
    "createdAt": "2026-01-28T05:56:19Z",
    "updatedAt": "2026-01-28T05:57:11Z",
    "author": "parallelarc",
    "authorAvatar": "https://avatars.githubusercontent.com/u/63178075?v=4",
    "authorUrl": "https://github.com/parallelarc",
    "labels": ["blog", "podcast"],
    "url": "https://github.com/parallelarc/parallelarc.github.io/issues/6",
    "commentsCount": 0,
    "excerpt": "印奇访谈决策备忘录 ---  基础模型商业模式  不可行的路径 - 基模+纯2B不成立：年投入30-50亿，但2B变现周期长、收入上限有限 - 基模+纯C端软件不成立：大厂拥有用户和数据飞轮，创业公司难以建立壁垒 - ChatGPT非终极产品形态，交互仍不自然  可行的路径 - AI与终端（人车家）..."
  }
]
